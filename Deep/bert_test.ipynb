{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete TensorFlow mixed-precision implementation with Bert\n",
    "\n",
    "*It seems that mixed-precision isn't working in this kernel, but it does locally with suitable graphics card (try it out!), and reduces the runtime by a factor of 2.<br><br>*\n",
    "*Update 1: Corrected the implementation, so that it now works as it should.<br><br>*\n",
    "*Update 2: Small adjustments, and added hyperparameters to transformer model<br><br>*\n",
    "*Update 3: `\" \".join(set(selected.lower().split()))`<br><br>*\n",
    "*Update 4: `len(text[i].split()) < 2: decoded_text = text`<br><br>*\n",
    "*Update 5: Removing softmax step on predictions<br><br>*\n",
    "*Update 6: Small fixes.<br><br>*\n",
    "*Update 7: corrected `input_type_ids`<br><br>*\n",
    "\n",
    "Done in three steps:\n",
    "1. define preprocessing procedure and add it to tf.data.Dataset.from_generator\n",
    "2. define Bert model together with train, predict and decode_prediction functions\n",
    "3. run the K-fold cross-validation including everything defined in step 1. and 2.\n",
    "\n",
    "Note: A simple **post processing** is used: predicting all 'neutral' sentiments to full text.\n",
    "\n",
    "credits to [abhishek](https://www.kaggle.com/abhishek) and all the 'get started' kernels to help/inspire me and many others to get started with this competition. The preprocessing in this notebook follows abhishek's [implementation](https://www.kaggle.com/abhishek/tweet-text-extraction-roberta-infer) with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil, floor\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from sklearn import model_selection\n",
    "from transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "tf.config.optimizer.set_jit(True)\n",
    "tf.config.optimizer.set_experimental_options(\n",
    "    {\"auto_mixed_precision\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (27480, 4)\n",
      "test shape  = (3534, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                I`d have responded, if I were going   neutral  \n",
       "1                                           Sooo SAD  negative  \n",
       "2                                        bullying me  negative  \n",
       "3                                     leave me alone  negative  \n",
       "4                                      Sons of ****,  negative  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                fun  positive  \n",
       "7                                         Soooo high   neutral  \n",
       "8                                        Both of you   neutral  \n",
       "9                       Wow... u just became cooler.  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv files\n",
    "train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df.loc[:, \"selected_text\"] = test_df.text.values\n",
    "\n",
    "submission_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "\n",
    "print(\"train shape =\", train_df.shape)\n",
    "print(\"test shape  =\", test_df.shape)\n",
    "\n",
    "# set some global variables\n",
    "PATH = \"../input/bert-base-uncased/\"\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "TOKENIZER = BertWordPieceTokenizer(f\"{PATH}/vocab.txt\", lowercase=True, add_special_tokens=False)\n",
    "\n",
    "# let's take a look at the data\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "I. Set up preprocessing and dataset/datagenerator\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(tweet, selected_text, sentiment):\n",
    "    \"\"\"\n",
    "    Will be used in tf.data.Dataset.from_generator(...)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # The original strings have been converted to \n",
    "    # byte strings, so we need to decode it\n",
    "    tweet = tweet.decode('utf-8')\n",
    "    selected_text = selected_text.decode('utf-8')\n",
    "    sentiment = sentiment.decode('utf-8')\n",
    "    \n",
    "    # Clean up the strings a bit\n",
    "    tweet = \" \".join(str(tweet).split())\n",
    "    selected_text = \" \".join(str(selected_text).split())\n",
    "    \n",
    "    # find the intersection between text and selected text\n",
    "    idx_start, idx_end = None, None\n",
    "    for index in (i for i, c in enumerate(tweet) if c == selected_text[0]):\n",
    "        if tweet[index:index+len(selected_text)] == selected_text:\n",
    "            idx_start = index\n",
    "            idx_end = index + len(selected_text)\n",
    "            break\n",
    "    \n",
    "    intersection = [0] * len(tweet)\n",
    "    if idx_start != None and idx_end != None:\n",
    "        for char_idx in range(idx_start, idx_end):\n",
    "            intersection[char_idx] = 1\n",
    "    \n",
    "    # tokenize with offsets\n",
    "    enc = TOKENIZER.encode(tweet)\n",
    "    input_ids_orig, offsets = enc.ids, enc.offsets\n",
    "\n",
    "    # compute targets\n",
    "    target_idx = []\n",
    "    for i, (o1, o2) in enumerate(offsets):\n",
    "        if sum(intersection[o1: o2]) > 0:\n",
    "            target_idx.append(i)\n",
    "    \n",
    "    target_start = target_idx[0]\n",
    "    target_end = target_idx[-1]\n",
    "\n",
    "    # add and pad data (hardcoded for BERT)\n",
    "    # --> [CLS] sentiment [SEP] input_ids [SEP] [PAD]\n",
    "    sentiment_map = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699,\n",
    "    }\n",
    "    \n",
    "    input_ids = [101] + [sentiment_map[sentiment]] + [102] + input_ids_orig + [102]\n",
    "    input_type_ids = [0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n",
    "    attention_mask = [1] * (len(input_ids_orig) + 4)\n",
    "    offsets = [(0, 0), (0, 0), (0, 0)] + offsets + [(0, 0)]\n",
    "    target_start += 3\n",
    "    target_end += 3\n",
    "\n",
    "    padding_length = MAX_SEQUENCE_LENGTH - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([0] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        input_type_ids = input_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "    elif padding_length < 0:\n",
    "        input_ids = input_ids[:padding_length-1] + [102]\n",
    "        attention_mask = attention_mask[:padding_length-1] + [1]\n",
    "        input_type_ids = input_type_ids[:padding_length-1] + [1]\n",
    "        offsets = offsets[:padding_length-1] + [(0, 0)]\n",
    "        if target_start >= MAX_SEQUENCE_LENGTH:\n",
    "            target_start = MAX_SEQUENCE_LENGTH - 1\n",
    "        if target_end >= MAX_SEQUENCE_LENGTH:\n",
    "            target_end = MAX_SEQUENCE_LENGTH - 1\n",
    "        \n",
    "    return (\n",
    "        input_ids, attention_mask, input_type_ids, offsets,\n",
    "        target_start, target_end, tweet, selected_text, sentiment, \n",
    "    )\n",
    "\n",
    "\n",
    "class TweetSentimentDataset(tf.data.Dataset):\n",
    "    \n",
    "    OUTPUT_TYPES = (\n",
    "        tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32, \n",
    "        tf.dtypes.int32,  tf.dtypes.float32, tf.dtypes.float32,\n",
    "        tf.dtypes.string, tf.dtypes.string,  tf.dtypes.string,\n",
    "    )\n",
    "    \n",
    "    OUTPUT_SHAPES = (\n",
    "        (MAX_SEQUENCE_LENGTH,),   (MAX_SEQUENCE_LENGTH,), (MAX_SEQUENCE_LENGTH,), \n",
    "        (MAX_SEQUENCE_LENGTH, 2), (),                     (),\n",
    "        (),                       (),                     (),\n",
    "    )\n",
    "    \n",
    "    # AutoGraph will automatically convert Python code to\n",
    "    # Tensorflow graph code. You could also wrap 'preprocess' \n",
    "    # in tf.py_function(..) for arbitrary python code\n",
    "    def _generator(tweet, selected_text, sentiment):\n",
    "        for tw, st, se in zip(tweet, selected_text, sentiment):\n",
    "            yield preprocess(tw, st, se)\n",
    "    \n",
    "    # This dataset object will return a generator\n",
    "    def __new__(cls, tweet, selected_text, sentiment):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=cls.OUTPUT_TYPES,\n",
    "            output_shapes=cls.OUTPUT_SHAPES,\n",
    "            args=(tweet, selected_text, sentiment)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(dataframe, batch_size, shuffle_buffer_size=-1):\n",
    "        dataset = TweetSentimentDataset(\n",
    "            dataframe.text.values, \n",
    "            dataframe.selected_text.values, \n",
    "            dataframe.sentiment.values\n",
    "        )\n",
    "\n",
    "        dataset = dataset.cache()\n",
    "        if shuffle_buffer_size != -1:\n",
    "            dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "II. Set up transformer model and functions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BertQAModel(TFBertPreTrainedModel):\n",
    "    \n",
    "    DROPOUT_RATE = 0.1\n",
    "    NUM_HIDDEN_STATES = 2\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        \n",
    "        self.bert = TFBertMainLayer(config, name=\"bert\")\n",
    "        self.concat = L.Concatenate()\n",
    "        self.dropout = L.Dropout(self.DROPOUT_RATE)\n",
    "        self.qa_outputs = L.Dense(\n",
    "            config.num_labels, \n",
    "            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "            dtype='float32',\n",
    "            name=\"qa_outputs\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # outputs: Tuple[sequence, pooled, hidden_states]\n",
    "        _, _, hidden_states = self.bert(inputs, **kwargs)\n",
    "        \n",
    "        hidden_states = self.concat([\n",
    "            hidden_states[-i] for i in range(1, self.NUM_HIDDEN_STATES+1)\n",
    "        ])\n",
    "        \n",
    "        hidden_states = self.dropout(hidden_states, training=kwargs.get(\"training\", False))\n",
    "        logits = self.qa_outputs(hidden_states)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    \n",
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(model, inputs, y_true, loss_fn, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(inputs, training=True)\n",
    "            loss  = loss_fn(y_true[0], y_pred[0])\n",
    "            loss += loss_fn(y_true[1], y_pred[1])\n",
    "            scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "    \n",
    "        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss, y_pred\n",
    "\n",
    "    epoch_loss = 0.\n",
    "    for batch_num, sample in enumerate(dataset):\n",
    "        loss, y_pred = train_step(\n",
    "            model, sample[:3], sample[4:6], loss_fn, optimizer)\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "        print(\n",
    "            f\"training ... batch {batch_num+1:03d} : \"\n",
    "            f\"train loss {epoch_loss/(batch_num+1):.3f} \",\n",
    "            end='\\r')\n",
    "        \n",
    "        \n",
    "def predict(model, dataset, loss_fn, optimizer):\n",
    "    \n",
    "    @tf.function\n",
    "    def predict_step(model, inputs):\n",
    "        return model(inputs)\n",
    "        \n",
    "    def to_numpy(*args):\n",
    "        out = []\n",
    "        for arg in args:\n",
    "            if arg.dtype == tf.string:\n",
    "                arg = [s.decode('utf-8') for s in arg.numpy()]\n",
    "                out.append(arg)\n",
    "            else:\n",
    "                arg = arg.numpy()\n",
    "                out.append(arg)\n",
    "        return out\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    offset = tf.zeros([0, MAX_SEQUENCE_LENGTH, 2], dtype=tf.dtypes.int32)\n",
    "    text = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    selected_text = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    sentiment = tf.zeros([0,], dtype=tf.dtypes.string)\n",
    "    pred_start = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n",
    "    pred_end = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n",
    "    \n",
    "    for batch_num, sample in enumerate(dataset):\n",
    "        \n",
    "        print(f\"predicting ... batch {batch_num+1:03d}\"+\" \"*20, end='\\r')\n",
    "        \n",
    "        y_pred = predict_step(model, sample[:3])\n",
    "        \n",
    "        # add batch to accumulators\n",
    "        pred_start = tf.concat((pred_start, y_pred[0]), axis=0)\n",
    "        pred_end = tf.concat((pred_end, y_pred[1]), axis=0)\n",
    "        offset = tf.concat((offset, sample[3]), axis=0)\n",
    "        text = tf.concat((text, sample[6]), axis=0)\n",
    "        selected_text = tf.concat((selected_text, sample[7]), axis=0)\n",
    "        sentiment = tf.concat((sentiment, sample[8]), axis=0)\n",
    "\n",
    "    # pred_start = tf.nn.softmax(pred_start)\n",
    "    # pred_end = tf.nn.softmax(pred_end)\n",
    "    \n",
    "    pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "        to_numpy(pred_start, pred_end, text, selected_text, sentiment, offset)\n",
    "    \n",
    "    return pred_start, pred_end, text, selected_text, sentiment, offset\n",
    "\n",
    "\n",
    "def decode_prediction(pred_start, pred_end, text, offset, sentiment):\n",
    "    \n",
    "    def decode(pred_start, pred_end, text, offset):\n",
    "\n",
    "        decoded_text = \"\"\n",
    "        for i in range(pred_start, pred_end+1):\n",
    "            decoded_text += text[offset[i][0]:offset[i][1]]\n",
    "            if (i+1) < len(offset) and offset[i][1] < offset[i+1][0]:\n",
    "                decoded_text += \" \"\n",
    "        return decoded_text\n",
    "    \n",
    "    decoded_predictions = []\n",
    "    for i in range(len(text)):\n",
    "        if sentiment[i] == \"neutral\" or len(text[i].split()) < 2:\n",
    "            decoded_text = text[i]\n",
    "        else:\n",
    "            idx_start = np.argmax(pred_start[i])\n",
    "            idx_end = np.argmax(pred_end[i])\n",
    "            if idx_start > idx_end:\n",
    "                idx_end = idx_start \n",
    "            decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n",
    "            if len(decoded_text) == 0:\n",
    "                decoded_text = text[i]\n",
    "        decoded_predictions.append(decoded_text)\n",
    "    \n",
    "    return decoded_predictions\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "III. Run it all: \n",
    "\n",
    "model.create() -> dataset.create() -> train(train) ->\n",
    "       -> predict(val).decode() -> predict(test).decode() -> submit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 01\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6849283694024291               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6967188469199268               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6959323610677993               \n",
      "\n",
      "fold 02\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6795086265175798               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.7022001777804603               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6930342607607635               \n",
      "\n",
      "fold 03\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6988943985589766               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.7017598677829847               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.697777264855256               \n",
      "\n",
      "fold 04\n",
      "\n",
      "epoch 001\n",
      "valid jaccard epoch 001: 0.6934241795915274               \n",
      "predicting ... batch 111                    \r\n",
      "epoch 002\n",
      "valid jaccard epoch 002: 0.6889684916940209               \n",
      "\n",
      "epoch 003\n",
      "valid jaccard epoch 003: 0.6889749848915699               \n"
     ]
    }
   ],
   "source": [
    "num_folds = 4\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n",
    "    optimizer, 'dynamic')\n",
    "\n",
    "config = BertConfig(output_hidden_states=True, num_labels=2)\n",
    "BertQAModel.DROPOUT_RATE = 0.2\n",
    "BertQAModel.NUM_HIDDEN_STATES = 2\n",
    "model = BertQAModel.from_pretrained(PATH, config=config)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(\n",
    "    n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# initialize test predictions\n",
    "test_preds_start = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "test_preds_end = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n",
    "\n",
    "for fold_num, (train_idx, valid_idx) in enumerate(\n",
    "        kfold.split(X=train_df.text, y=train_df.sentiment.values)):\n",
    "    print(\"\\nfold %02d\" % (fold_num+1))\n",
    "        \n",
    "    train_dataset = TweetSentimentDataset.create(\n",
    "        train_df.iloc[train_idx], batch_size, shuffle_buffer_size=2048)\n",
    "    valid_dataset = TweetSentimentDataset.create(\n",
    "        train_df.iloc[valid_idx], batch_size, shuffle_buffer_size=-1)\n",
    "    test_dataset = TweetSentimentDataset.create(\n",
    "        test_df, batch_size, shuffle_buffer_size=-1)\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    for epoch_num in range(num_epochs):\n",
    "        print(\"\\nepoch %03d\" % (epoch_num+1))\n",
    "        \n",
    "        # train for an epoch\n",
    "        train(model, train_dataset, loss_fn, optimizer)\n",
    "        \n",
    "        # predict validation set and compute jaccardian distances\n",
    "        pred_start, pred_end, text, selected_text, sentiment, offset = \\\n",
    "            predict(model, valid_dataset, loss_fn, optimizer)\n",
    "        \n",
    "        selected_text_pred = decode_prediction(\n",
    "            pred_start, pred_end, text, offset, sentiment)\n",
    "        jaccards = []\n",
    "        for i in range(len(selected_text)):\n",
    "            jaccards.append(\n",
    "                jaccard(selected_text[i], selected_text_pred[i]))\n",
    "        \n",
    "        score = np.mean(jaccards)\n",
    "        print(f\"valid jaccard epoch {epoch_num+1:03d}: {score}\"+\" \"*15)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            # requires you to have 'fold-{fold_num}' folder in PATH:\n",
    "            # model.save_pretrained(PATH+f'fold-{fold_num}')\n",
    "            # or\n",
    "            # model.save_weights(PATH + f'fold-{fold_num}.h5')\n",
    "            \n",
    "            # predict test set\n",
    "            test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n",
    "                predict(model, test_dataset, loss_fn, optimizer)\n",
    "    \n",
    "    # add epoch's best test preds to test preds arrays\n",
    "    test_preds_start += test_pred_start\n",
    "    test_preds_end += test_pred_end\n",
    "    \n",
    "    # reset model, as well as session and graph (to avoid OOM issues?) \n",
    "    session = tf.compat.v1.get_default_session()\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    del session, graph, model\n",
    "    model = BertQAModel.from_pretrained(PATH, config=config)\n",
    "    \n",
    "# decode test set and add to submission file\n",
    "selected_text_pred = decode_prediction(\n",
    "    test_preds_start, test_preds_end, test_text, test_offset, test_sentiment)\n",
    "\n",
    "submission_df.loc[:, 'selected_text'] = selected_text_pred\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
