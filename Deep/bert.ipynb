{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 14 - Machine Learning Spring'2020 Final Project\n",
    "### Initial TensorFlow BERT Approach to Tweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#tensorflow imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tokenizers\n",
    "#Get the BERT text tokenizer and associated model for tensorflow\n",
    "from transformers import BertTokenizer, BertConfig, TFBertModel\n",
    "#tqdm to show progress throughout iterations\n",
    "from tqdm import tqdm\n",
    "#regex library\n",
    "import re\n",
    "\n",
    "#Allow support for loading bars in Pandas - this is just helpful\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write locations of where each piece of data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Competition data inside the Kaggle kernel is located inside tweet-sentiment-extraction\n",
    "#size for validation set- exclude from training set\n",
    "size = int(0.2 * len(train))\n",
    "DATA = \"\"\n",
    "#load training set\n",
    "train = pd.read_csv(DATA+'../train.csv')\n",
    "train = train[:-size]\n",
    "#load testing set\n",
    "test = pd.read_csv(DATA+'../test.csv')\n",
    "#load sample submission to get the format for the final data submission\n",
    "submission = pd.read_csv(DATA+'../sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class for the model's configuration, that way it can be passed easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    #Max length of a tweet is 128\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    VALID_BATCH_SIZE = 32\n",
    "    TEST_BATCH_SIZE = 32\n",
    "    EPOCHS = 10\n",
    "    #Add the location of the model's UNCAPTIALIZED configuration\n",
    "    BERT_CONFIG = '/kaggle/input/bertconfig/bert-base-uncased-config.json'\n",
    "    BERT_PATH = \"/kaggle/input/bert-base-uncased-huggingface-transformer/\"\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(\"/kaggle/input/bert-base-uncased-huggingface-transformer//bert-base-uncased-vocab.txt\", \n",
    "        lowercase=True)\n",
    "    SAVEMODEL_PATH = '/kaggle/input/tftweetfinetuned/finetuned_bert.h5'\n",
    "    THRESHOLD = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a tweet and the training selected text, create a method to process that tweet and tokenize it for analysis. Tokenizers help train models on new vocabulary. We referenced [this Tokenizers library](https://github.com/huggingface/tokenizers/tree/master/bindings/python) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, tokenizer):\n",
    "    len_st = len(selected_text)\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "    \n",
    "    #Go through the tweet and its selected text and see where the common words exist\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
    "        if tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st\n",
    "            break\n",
    "    \n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1):\n",
    "            char_targets[ct] = 1\n",
    "    #Tokenize the string\n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "\n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    #Find target words and return them\n",
    "    targets = [0] * len(input_ids_orig)\n",
    "    for idx in target_idx:\n",
    "        targets[idx] = 1\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another step in our preprocessing pipeline - try to remove extraneous data that is completely irrelevant to our model's classifiyng work.\n",
    "This includes any emojis as well as hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(tweet):\n",
    "    #list of emoji patterns appearing in the tweets to be removed\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = str(tweet)\n",
    "    # Remove emojis\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # Remove twitter handles (@___)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove links after research that t.co uses http still\n",
    "    text = re.sub(r'http.?://[^/s]+[/s]?', '', text)\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: cleanText(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our validation methods, we chose a 20% set of the training data. Given word info and a set of data to validate with, compute the average Jaccard score provided by the words selected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(tweets, sents):\n",
    "    vocab = dict()\n",
    "    word_id = 0\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        \n",
    "        # Skip words in neutral tweets\n",
    "        if sents[i] == 'neutral':\n",
    "            continue\n",
    "        \n",
    "        tweet = tweets[i]\n",
    "        for word in tweet.split():\n",
    "            # Check if new word\n",
    "            if word not in vocab:\n",
    "                # Add to vocab dictionary with a unique ID number\n",
    "                vocab[word] = word_id\n",
    "                word_id += 1\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given word info and a set of data to validate with, compute the average\n",
    "# Jaccard score provided by the words selected by the model.\n",
    "def validate(vocab, tweets, selects, sents):\n",
    "    n = len(tweets)\n",
    "    \n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        true_selected = selects[i]\n",
    "        pred_selected = select_words(vocab, tweets[i], sents[i])\n",
    "        total += jaccard(true_selected, pred_selected)\n",
    "    \n",
    "    return total / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set extraction here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(0.2 * len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the validation set in now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole tweet\n",
    "v_train = pd.read_csv('../train.csv')\n",
    "v_tweet = np.array(v_train.iloc[size:, 1])\n",
    "#selected tweet\n",
    "v_select = np.array(v_train.iloc[size:, 2])\n",
    "#sentiment of tweet\n",
    "v_sen = np.array(v_train.iloc[size:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this method to the training dataset - create a new column called targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['targets'] = train.progress_apply(lambda row: process_data(   str(row['text']), \n",
    "                                                                    str(row['selected_text']),\n",
    "                                                                    config.TOKENIZER),\n",
    "                                                                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the targets in the event of variant length tweets. Padding is a technique in NLP that ensures that the length of the string doesn't make an impact on its classification. We referenced [this website](https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/) to learn more about this topic. Essentially all tweets in the dataset have their lengths padded with \"dummy variables\" to ensure they look the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['targets'] = train['targets'].apply(lambda x :x + [0] * (config.MAX_LEN-len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data, convert it to the form that the BERT Transformer expects from the targets column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_transformer_inputs(text, tokenizer, max_sequence_length):\n",
    "    inputs = tokenizer.encode(text)\n",
    "    input_ids =  inputs.ids\n",
    "    input_masks = inputs.attention_mask\n",
    "    input_segments = inputs.type_ids\n",
    "    padding_length = max_sequence_length - len(input_ids)\n",
    "    padding_id = 0\n",
    "    input_ids = input_ids + ([padding_id] * padding_length)\n",
    "    input_masks = input_masks + ([0] * padding_length)\n",
    "    input_segments = input_segments + ([0] * padding_length)\n",
    "    return [input_ids, input_masks, input_segments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous method,calculate the inner arrays of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_input_arrays(df, tokenizer, max_sequence_length):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for _, instance in tqdm(df.iterrows()):\n",
    "        ids, masks, segments= convert_to_transformer_inputs(str(instance.text),tokenizer, max_sequence_length)\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "        \n",
    "    return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previous methods to perform the operations on the training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = compute_output_arrays(train,'targets')\n",
    "inputs = compute_input_arrays(train, config.TOKENIZER, config.MAX_LEN)\n",
    "test_inputs = compute_input_arrays(test, config.TOKENIZER, config.MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation using the Configuration class defined above. We use the sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    ids = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n",
    "    mask = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n",
    "    attn = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n",
    "    bert_conf = BertConfig() \n",
    "    bert_model = TFBertModel.from_pretrained(config.BERT_PATH+'/bert-base-uncased-tf_model.h5', config=bert_conf)\n",
    "    \n",
    "    output = bert_model(ids, attention_mask=mask, token_type_ids=attn)\n",
    "    \n",
    "    out = tf.keras.layers.Dropout(0.1)(output[0]) \n",
    "    out = tf.keras.layers.Conv1D(1,1)(out)\n",
    "    out = tf.keras.layers.Flatten()(out)\n",
    "    out = tf.keras.layers.Activation('sigmoid')(out)\n",
    "    model = tf.keras.models.Model(inputs=[ids, mask, attn], outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with keras. We are using the Binary cross entropy as the loss function as our classes (positive/negative) are such polar opposites that it would be a good fit. Neutral class in general gets the whole message sent back so it's not an issue for us. Furthermore, other loss functions like Categorical Cross entropy tend to have extremely high loss function in testing, so we went with this.\n",
    "Calculating the learning rate was challenging for this task but we went with 0.00005 as we can't have too many epochs with this training (it just takes too much time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model, and save the model that was just created out to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(config.SAVEMODEL_PATH):\n",
    "    model.fit(inputs,outputs, epochs=config.EPOCHS, batch_size=config.TRAIN_BATCH_SIZE)\n",
    "    model.save_weights(f'finetuned_bert.h5')\n",
    "else:\n",
    "    model.load_weights(config.SAVEMODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction time. Now we input the testing dataset and work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_inputs, batch_size=32, verbose=1)\n",
    "threshold = config.THRESHOLD\n",
    "pred = np.where(predictions>threshold, 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation of generating the submission csv, take the dataset and decode each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tweet(original_tweet,idx_start,idx_end,offsets):\n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "    return filtered_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through the testing set and decode the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for test_idx in range(test.shape[0]):\n",
    "    indexes = list(np.where(pred[test_idx]==1)[0])\n",
    "    text = str(test.loc[test_idx,'text'])\n",
    "    encoded_text = config.TOKENIZER.encode(text)\n",
    "    if len(indexes)>0:\n",
    "        start = indexes[0]\n",
    "        end =  indexes[-1]\n",
    "    else:  #if nothing was found above threshold value\n",
    "        start = 0\n",
    "        end = len(encoded_text.ids) - 1\n",
    "    if end >= len(encoded_text.ids):\n",
    "        end = len(encoded_text.ids) - 1\n",
    "    if start>end: \n",
    "        selected_text = test.loc[test_idx,'text']\n",
    "    else:\n",
    "        selected_text = decode_tweet(text,start,end,encoded_text.offsets)\n",
    "    outputs.append(selected_text)\n",
    "    \n",
    "test['selected_text'] = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps handle the case of the neutral tweets where the selected text is most often the actual tweet itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(row):\n",
    "    if row['sentiment'] == 'neutral' or len(row['text'].split())<2:\n",
    "        return row['text']\n",
    "    else:\n",
    "        return row['selected_text']\n",
    "test['selected_text'] = test.apply(replacer,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the submission csv used for turning into Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['selected_text'] = test['selected_text']\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_score = validate(v_train, )\n",
    "print(\"Validation set score: , \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
