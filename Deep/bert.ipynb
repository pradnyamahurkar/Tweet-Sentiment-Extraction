{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Group 14 - Machine Learning Spring'2020 Final Project\n### TensorFlow BERT Approach to Tweet Sentiment Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import the necessary libraries\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\n#tensorflow imports\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\n#Get the BERT text tokenizer and associated model for tensorflow\nfrom transformers import BertTokenizer, BertConfig, TFBertModel\n#tqdm to show progress throughout iterations\nfrom tqdm import tqdm\n\n#Allow support for loading bars in Pandas - this is just helpful\ntqdm.pandas()","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write locations of where each piece of data is."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Competition data inside the Kaggle kernel is located inside tweet-sentiment-extraction\n#Global variables in python are capitalized\nDATA = \"/kaggle/input/tweet-sentiment-extraction/\"\n#load training set\ntrain = pd.read_csv(DATA+'train.csv')\n#load testing set\ntest = pd.read_csv(DATA+'test.csv')\n#load sample submission to get the format for the final data submission\nsubmission = pd.read_csv(DATA+'sample_submission.csv')","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a class for the model's configuration, that way it can be passed easily."},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n    #Max length of a tweet is 128\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 32\n    TEST_BATCH_SIZE = 32\n    EPOCHS = 10\n    #Add the location of the model's UNCAPTIALIZED configuration\n    BERT_CONFIG = '/kaggle/input/bertconfig/bert-base-uncased-config.json'\n    BERT_PATH = \"/kaggle/input/bert-base-uncased-huggingface-transformer/\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\"/kaggle/input/bert-base-uncased-huggingface-transformer//bert-base-uncased-vocab.txt\", \n        lowercase=True)\n    SAVEMODEL_PATH = '/kaggle/input/tftweetfinetuned/finetuned_bert.h5'\n    THRESHOLD = 0.4","execution_count":52,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given a tweet and the training selected text, create a method to process that tweet and tokenize it for analysis. Tokenizers help train models on new vocabulary. We referenced [this Tokenizers library](https://github.com/huggingface/tokenizers/tree/master/bindings/python) to learn more."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def process_data(tweet, selected_text, tokenizer):\n    len_st = len(selected_text)\n    idx0 = None\n    idx1 = None\n    \n    #Go through the tweet and its selected text and see where the common words exist\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st\n            break\n    \n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1):\n            char_targets[ct] = 1\n    #Tokenize the string\n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    #Find target words and return them\n    targets = [0] * len(input_ids_orig)\n    for idx in target_idx:\n        targets[idx] = 1\n    return targets","execution_count":53,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now apply this method to the training dataset - create a new column called targets\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['targets'] = train.progress_apply(lambda row: process_data(   str(row['text']), \n                                                                    str(row['selected_text']),\n                                                                    config.TOKENIZER),\n                                                                    axis=1)","execution_count":54,"outputs":[{"output_type":"stream","text":"100%|██████████| 27481/27481 [00:03<00:00, 8531.54it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Pad the targets in the event of variant length tweets. Padding is a technique in NLP that ensures that the length of the string doesn't make an impact on its classification. We referenced [this website](https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/) to learn more about this topic. Essentially all tweets in the dataset have their lengths padded with \"dummy variables\" to ensure they look the same. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['targets'] = train['targets'].apply(lambda x :x + [0] * (config.MAX_LEN-len(x)))","execution_count":55,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the data, convert it to the form that the BERT Transformer expects from the targets column."},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_transformer_inputs(text, tokenizer, max_sequence_length):\n    inputs = tokenizer.encode(text)\n    input_ids =  inputs.ids\n    input_masks = inputs.attention_mask\n    input_segments = inputs.type_ids\n    padding_length = max_sequence_length - len(input_ids)\n    padding_id = 0\n    input_ids = input_ids + ([padding_id] * padding_length)\n    input_masks = input_masks + ([0] * padding_length)\n    input_segments = input_segments + ([0] * padding_length)\n    return [input_ids, input_masks, input_segments]","execution_count":56,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the previous method,calculate the inner arrays of the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input_arrays(df, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df.iterrows()):\n        ids, masks, segments= convert_to_transformer_inputs(str(instance.text),tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_output_arrays(df, columns):\n    return np.asarray(df[columns].values.tolist())","execution_count":58,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the previous methods to perform the operations on the training and testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays(train,'targets')\ninputs = compute_input_arrays(train, config.TOKENIZER, config.MAX_LEN)\ntest_inputs = compute_input_arrays(test, config.TOKENIZER, config.MAX_LEN)","execution_count":59,"outputs":[{"output_type":"stream","text":"27481it [00:06, 4574.51it/s]\n3534it [00:00, 4871.76it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Model creation using the Configuration class defined above."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    ids = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    mask = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    attn = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    bert_conf = BertConfig() \n    bert_model = TFBertModel.from_pretrained(config.BERT_PATH+'/bert-base-uncased-tf_model.h5', config=bert_conf)\n    \n    output = bert_model(ids, attention_mask=mask, token_type_ids=attn)\n    \n    out = tf.keras.layers.Dropout(0.1)(output[0]) \n    out = tf.keras.layers.Conv1D(1,1)(out)\n    out = tf.keras.layers.Flatten()(out)\n    out = tf.keras.layers.Activation('sigmoid')(out)\n    model = tf.keras.models.Model(inputs=[ids, mask, attn], outputs=out)\n    return model","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the model with keras. We are using the Binary cross entropy as the loss function as our classes (positive/negative) are such polar opposites that it would be a good fit. Neutral class in general gets the whole message sent back so it's not an issue for us. Furthermore, other loss functions like Categorical Cross entropy tend to have extremely high loss function in testing, so we went with this.\nCalculating the learning rate was challenging for this task but we went with 0.00005 as we can't have too many epochs with this training (it just takes too much time)."},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\nmodel = create_model()\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer)","execution_count":61,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit the model, and save the model that was just created out to a file."},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.exists(config.SAVEMODEL_PATH):\n    model.fit(inputs,outputs, epochs=config.EPOCHS, batch_size=config.TRAIN_BATCH_SIZE)\n    model.save_weights(f'finetuned_bert.h5')\nelse:\n    model.load_weights(config.SAVEMODEL_PATH)","execution_count":null,"outputs":[{"output_type":"stream","text":"Train on 27481 samples\nEpoch 1/10\n19072/27481 [===================>..........] - ETA: 1:56 - loss: 0.0243","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Prediction time. Now we input the testing dataset and work with that."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_inputs, batch_size=32, verbose=1)\nthreshold = config.THRESHOLD\npred = np.where(predictions>threshold, 1,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In preparation of generating the submission csv, take the dataset and decode each tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_tweet(original_tweet,idx_start,idx_end,offsets):\n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n    return filtered_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run through the testing set and decode the tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = []\nfor test_idx in range(test_df.shape[0]):\n    indexes = list(np.where(pred[test_idx]==1)[0])\n    text = str(test_df.loc[test_idx,'text'])\n    encoded_text = config.TOKENIZER.encode(text)\n    if len(indexes)>0:\n        start = indexes[0]\n        end =  indexes[-1]\n    else:  #if nothing was found above threshold value\n        start = 0\n        end = len(encoded_text.ids) - 1\n    if end >= len(encoded_text.ids):\n        end = len(encoded_text.ids) - 1\n    if start>end: \n        selected_text = test_df.loc[test_idx,'text']\n    else:\n        selected_text = decode_tweet(text,start,end,encoded_text.offsets)\n    outputs.append(selected_text)\n    \ntest_df['selected_text'] = outputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helps handle the case of the neutral tweets where the selected text is most often the actual tweet itself. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def replacer(row):\n    if row['sentiment'] == 'neutral' or len(row['text'].split())<2:\n        return row['text']\n    else:\n        return row['selected_text']\ntest_df['selected_text'] = test_df.apply(replacer,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the submission csv used for turning into Kaggle."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df['selected_text'] = test_df['selected_text']\nsubmission_df.to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 80)\nsubmission_df.sample(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}